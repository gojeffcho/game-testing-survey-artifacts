Years Active in Games Industry,Roles Held,Years of Indie Exp,Years of AA Exp,Years of AAA Exp,Age,Country,Type,Primary Role,Other Roles,Engine,Time On Project,Team Size,Budget,Publisher,Project Duration,Test Performance Narrative,Performance 1,Performance 2,Performance 3,Performance 4,Performance 5,Performance 6,Performance 7,Performance 8,Test Planning Narrative,Planning 1,Planning 2,Planning 3,Planning 4,Planning 5,Planning 6,Planning 7,Planning 8,Planning 9,Test Goals,Goals 1,Goals 2,Goals 3,Goals 4,Test Automation Narrative,Automation 1,Automation 2,Automation 3,Automation 4,Automation 5,Automation 6,Automation 7,Automation 8,Automation 9,Automation 10,Test Tools,Tools 1,Tools 2,Tools 3,Tools 4,Tools 5,Tools 6,Test Output Narrative,Output 1,Output 2,Output 3,Output 4,Output 5,Output 6,Test Resources,Resources 1,Resources 2,Resources 3,Resources 4,Resources 5,Resources 6,Resources 7,Resources 8,Resources 9,Additional Info,QA - Helpful,QA - Painful,QA - Wants
10,"['Lead/Prod/Mgr', 'Designer/ProdOwner']",6,0,4,25-34,Canada,Indie Game,"Team Lead, Producer, and/or Project Manager","['Designer/ProdOwner', 'Sound Designer']",Unity,Over 3 years,1 - 49,Under $1M USD,"No, we did not have a publisher",Over 3 years,"Everyone did initial work on features and content independently. We would do testing on features and content as we worked. We would then send our feature/content to all other team members who would play and test the content on their own. Any bugs or issues that popped up would be showed to the feature/content owner with reproduction steps. When all bugs from that round had been fixed, it would be sent out to team members again to see if they could reproduce the original bugs or find new ones. This would continue until no more bugs were found. At that point we would do playtesting with external players, either through focus tests or by deploying the content to customers. If new bugs were found, at that point, the cycle would start again.",1,1,5,1,1,4,4,4,We made no test plans. We were a small team and were each responsible for testing our own content. This was not done through a formal process.,1,1,1,1,5,4,1,1,1,"We didn't want to show buggy features or content to other developers or to release buggy features or content to our players. Because the onus for testing fell onto individual developers to show other developers bug free features and content, very few bugs reached other developers, and even fewer still made it to players. Most of the bugs that reached players had to do with hardware that was unavailable to the development team, or design bugs that developers never considered.",3,3,5,5,We did no automated testing. All testing was manual. It was done first by the primary feature/content developer and then by the other developers.,5,1,0,0,0,-1,0,5,2,0,"We used no tools for testing. It was all done in Engine. So I guess the Engine was the testing tool. We also had no real testing goals, so... ¯\_(ツ)_/¯",4,0,4,0,0,4,"Testing output was verbal, and demonstrated in person or via recorded video with voice over.",4,5,4,5,0,1,"We used our own time, since we had neither additional human resources, funding, or hardware to dedicate to testing. It was expected of each developer that they had done adequate testing before putting a feature forward.",4,3,2,3,1,1,5,5,0,"It worked really well? We were complemented by critical and player reviews for having a pretty bug free game. I think a key to this was that it was taken as a given that everyone should take the time to test and iterate on their features before pushing them out to other people. In my other projects, people say to do that, but then rarely actually give you the space, or expect you to keep up with your task backlog in order to do that in an effective manner.",Automated tests,Maintaining automated tests,Dedicated infrastructure to help developers write effective automated tests.
2,"['Lead/Prod/Mgr', 'Designer/ProdOwner']",2,0,0,25-34,United States,Indie Game,"Team Lead, Producer, and/or Project Manager",['Designer/ProdOwner'],Custom Engine,1 year or less,1 - 49,Under $1M USD,"Yes, we had a publisher who provided marketing support",2 - 3 years,"We had a full QA team through the publisher that would regularly do Smoke, Regression, and CQA tests.",3,3,4,5,3,5,4,5,The test plans were made by myself and checked by the QA Manager,5,4,4,1,1,2,3,2,5,Mainly checking player interactions and server issues.,5,5,5,3,All testing was manual.,5,0,0,0,0,-1,0,-1,0,0,We used a specific engine and set of tools made by one of our team members for this project.,5,3,3,5,5,5,testing data is extracted into charts and summaries at my org so we can see the long-term progression of a project.,5,5,4,4,2,5,"We had QA staff, dev kits, and 25k in the budget for QA.",3,2,1,2,5,1,5,4,2,,Have a really organized and specific test plan.,Testers not staying focused on the exact things we want reported at a given time.,I haven't used many automation tools so I am curious about that now.
2,['UX Researcher'],0,1,0,25-34,Canada,AA Game,UX Researcher,['n/a'],Unreal,1 year or less,100 - 499,Unknown,"Yes, we had a publisher, but they did not provide marketing support",Over 3 years,"Internal testing was led by QA and had other members of the company/team play the game, specifically for any new features or to figure out any known bugs. Most feedback was focused on the goal for that session. This was done multiple times a week. There was also focus testing and alpha testing weekends done with friends/family of the project - players were free to play through the current version of the game over the weekend and submit any feedback or bugs they had. Focus testing was done to observe player behaviors as they played through the game, or a specific aspect of the game, and to interview players to get deeper thoughts on what was working well/what wasn't. Multiple players were usually focus tested at once. This was done every few months, or monthly for smaller focus testing groups. ",-1,-1,4,5,1,5,5,5,"Various team members (such as designers, producers, etc.) would come to the QA team to let them know what they wanted tested for the upcoming week for internal testing. QA would need specifics such as what was being tested, instructions for other team members who would be doing the testing, and if there were any specific build needs. UX research was reached out to if a survey was needed for the test, which was also handed off to QA to distribute. This was done on a weekly basis, with specific goal needs becoming more important and more prominent in testing towards our release dates. External focus tests were also done on a monthly/bi-monthly basis, with goals being set out by the production team for what was to be tested. Specific interview and survey questions were developed by UX research. ",-1,4,1,4,1,4,3,2,2,"On a regular basis we would be testing out the build to see if there were any regressions, bugs, or any new issues that came with new features that were being implemented to stay on top of issues with the build. On a larger basis, with the external playtests, we were looking for player attitudes and seeing if players were enjoying the game, understanding it, and if there were any issues they were running into that we were not aware of - whether that be bugs or unclear features.  By having a very regular and consistent testing schedule, it helped us find any bugs or issues early on, and also to help us make sure that everything was on track for our desired game experience (which was really helpful with the external tests). ",4,5,5,1,"I actually don't know if there was automated testing on the project or what it was used for, but I think there was some automation? I'm unclear what the automation was used for if it was there. As mentioned, manual testing was done almost daily on the project for the whole team, with QA working in the builds every day as well. User experience, user bugs, and integration of features seemed to be the prioritized aspects of manual testing.",-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,Unsure.,-1,-1,-1,-1,-1,5,Unsure.,5,5,4,2,4,-1,We had a whole internal QA team consisting of around 10 people that were dedicated towards testing and build management. Many QA testers would stay late leading up to a release date to ensure that everything was working properly. Many developers and other team members would also allocate part of their day to doing some internal testing. I am less sure about hardware. There was one UX researcher dedicated to external testing.,2,5,-1,-1,-1,1,5,2,4,,"Knowing clear goals, understanding the testing process, standardized documentation.","Unclear testing goals and crunch leading up to testing. This usually stems from a disorganized lead up to release, leading to all of my work needing to be done right before external testing is to be done. Ensuring results are accurate and clear.","More diverse representation from external players chosen for testing, more integration of UX and UX research into the testing and design process (rather than being seen as a completely separate function), and more documentation for UX research/testing in games."
5,"['QA', 'Designer/ProdOwner']",5,2,0,25-34,United States,Indie Game,"Game Designer, Level Designer, Gameplay Designer, and/or Product Owner",['QA'],Unity,2 - 3 years,1 - 49,Under $1M USD,"No, we did not have a publisher",2 - 3 years,"We all kinda did our part, we were still inexperienced so it involved a lot of trial and error alongside creativity. Our small team didn't mind the extra time. Honestly it was something we looked forward to. We all kinda liked thinking of ways to break the game and we felt the testing brought us a lot of positives. Including the scope and size of what we wanted vs what was realistic, but as we grew to know what we had we learned what we else we could do within reason.",4,3,5,2,3,4,4,5,"At the time, we needed it before we realized it. After that hill we usually ran tests after implementations of new mechanics and designs",5,4,4,4,4,2,2,2,2,"We were really looking for hardware compatibility, making sure it'd run across the board. Making it run smoothly, optimizing and fixing the little things as we went",5,5,5,4,"Since we were a small team,  we did a lot of it ourselves. We all knew the game very well and our artists/testers/programmers were always communicating",5,2,3,4,2,5,3,-1,2,4,"Our game was pretty small, automated testing wasn't really necessary because often we knew where the problems were. It was more of a safety net for unseen things we didn't catch",3,2,2,4,4,4,"We had a group discord, when things came up they were put there. We liked to call it the ""explain it like I'm 5"" rule. We tried to simplify our problems so that just about everyone on our team could understand why what was doing what.",5,2,4,2,2,3,"We all kinda did our parts when we could. We felt like if there was a problem, we took them on as a team. Our artists believe it or not were often our MVPs. It was something we all believed in and we wanted it to be the best it could be",4,2,2,2,2,2,5,5,5,,"Proper means of communication, being able to openly ask questions without ""feeling dumb"". We were all equal and we were all in it together.","I think at first it was a lack of knowledge of how to properly test, what resources were available and learning as we went.","I wish it was easier to find outside eyes to look in. We were a small team, didn't really know anyone in the business and I wish it was easier to just find a little help. We didn't need a whole department sometimes, just a few fresh eyes here and there."
6,"['Dev/Prog/Eng', 'QA', 'Lead/Prod/Mgr', 'Designer/ProdOwner', 'Business, marketing']",6,0,0,25-34,Canada,Indie Game,"Developer, Programmer, and/or Engineer","['QA', 'Lead/Prod/Mgr', 'Designer/ProdOwner', 'Business, marketing, writer']",Unity,Over 3 years,1 - 49,Under $1M USD,"No, we did not have a publisher",Over 3 years,"I would personally test new features and make sure they're broadly working as I implemented them.    I and two other team members would periodically play through portions of the game and take note of balance issues and bugs. We'd record these in Trello. Some issues of balance and level design would be corrected by our combat/level designer, but generally correcting the bugs would come to me. Again, once the fix was implemented, I'd do an initial round of tests to confirm it, and would sometimes ask the other team members to keep an eye out for tricky cases.    We did not specifically solicit player feedback except for some demonstrations at events. We observed players interacting with the game and made some adjustments based on their reactions.    All this was done during development without any particular organization. Once we got to the point that all features and content were implemented, we went through a couple of rounds of more organized testing. We would play through the entire game, plus individual sections, noting every correction we thought was needed. I would review these issues and correct them. We then did more iterations of this process until we no longer found significant bugs. We shipped with a couple of minor bugs that were corrected later.    After release we adjusted some additional balance issues based on player feedback, and built a couple of additional features as we worked toward a content add-on.",4,2,5,2,5,3,4,4,"I would routinely make test plans based on what was currently being implemented. They would generally only involve me. The exception would be issues that occurred relatively rarely and did not immediately appear to be reliably reproducible. To help understand the issue, I would ask the other developers to watch for them and let me know the circumstances.    For our more dedicated testing phases, I would draw up a list of activities I felt could potentially produce errors, and while playing through the game as a whole or in parts, would, along with the other developers, check them off.",2,4,4,2,3,4,2,4,5,"With our limited resources, the main priority was game-breaking bugs. To our knowledge, no version shipped with any game-breaking bugs, so it seems we were successful here. That said, one particular issue did only get caught right before release on our very last ""just in case"" round of testing.    Our other priorities were other miscellaneous bugs, along with balance and not having weird out-of-bounds level issues. We definitely released the game a bit harder than it should have been, probably because we got too used to the systems and didn't have a good idea about how someone coming to it fresh would experience it.",4,5,2,3,It was pretty much all manual. I think I tried one automated tool but it didn't play well with the laptop I was using for development at the time. I also wasn't sure if taking the time to learn such tools would ultimately pay off with the limited resources. As such I don't think I can really offer much of an opinion on automated testing.,5,1,0,0,0,0,0,0,0,0,The only tools used were developed as internal parts of the application. Unless you count Trello for tracking.,0,0,0,0,0,2,"When we found a bug or other issue, we'd put it on a Trello board. As I or others were investigating, we'd move it to a pending list. When done, we'd move it to a done list. After some time passes and we're sure the issue is fixed, it got archived from there.    Notes would usually be quite short, but we wanted to capture what the issue was, and the circumstances that seem to trigger it, if relevant.",5,4,4,3,4,2,"As a team of four and a half, there's huge blurry lines in roles. Testing was basically something that was done in tandem with everything else, before becoming the major dedicated last few steps. It's hard to pull it out in any better terms than that, unfortunately. We used the same hardware we used for development.",4,2,0,4,2,1,5,5,0,"Just as extra context, the project had no formal full-time roles. We had one musician and one level/combat designer who both helped with testing. We had an artist who did not. We had a programmer who contributed some minor things (no testing), and we had two programmers who left the project very early. I did basically everything else.","Getting stuff on Trello was a good step and met our needs as a small team. Honestly, I also just used pen and paper a lot when initially finding issues.",The classic: fixing one thing creates new issues you had no idea could exist.,"Hard to say with my experience being on a small team. For my part, I would want to more systematically get player feedback for future projects."
11,"['Dev/Prog/Eng', 'Lead/Prod/Mgr', 'Manager, Engineering']",3,0,7,25-34,USA,AAA Game,"Developer, Programmer, and/or Engineer",['Lead/Prod/Mgr'],Unity,2 - 3 years,100 - 499,Over $10M USD,"No, we did not have a publisher",Over 3 years,"Internal QA resources worked with feature teams to validate new features.  Resources from parent company's QA office performed release verifications and regression testing, consisting of walking through every feature in the game prior to an update (weekly)  Internal QA resources worked with designers to understand upcoming features and maintain the test plans.  Engineers maintained unit and regression test suites for automated nightly tests.",1,5,4,4,5,5,5,5,"Test plans made and maintained by internal QA in test rails. Guidance on expected behavior of features provided by the features primary designer.    Engineering group separately implemented unit and regression tests to their understanding of the feature.    Editorial: the below questions are not well formed. QA was responsible for the test plans, but did not control impacts to release or priorities. QA had input in release decisions, but the product team and studio GM maintained authority. Though the product group didn't generally concern themselves with the test plan unless/until there were live issues.    It was not unusual for the tests to be built along with the feature, especially where features were not fully defined prior to implementation.",5,5,5,3,1,4,3,4,5,New features function as expected. Prior features continue to function as expected. Performance maintains within expected levels across supported devices. Infrastructure is able to support anticipated load.,5,5,4,2,"This question feels malformed. Ownership and maintenance of manual and automated tests were disjoint.    Manual testing generally covered the product in gestalt. Automated testing generally covered the features of the product.    Some common and tedious manual tests additionally had staffing put forward to automate. This required buy in from engineering and QA leadership, in part due to the significant cost gaps (both financial as well as opportunity) between engineer time and QA time.",3,4,1,3,2,2,2,5,3,1,Test rails  Nunit  Unity test framework  FsCheck  Tsung,1,3,5,5,5,4,Automated reports output based on tools default format through CI.  Manual reports generated Jira tickets.,4,4,4,5,2,4,"Small internal QA team, primarily focused on upcoming features. Single dedicated internal release QA. External QA available for support.",4,4,4,5,5,2,4,1,5,Mobile/f2p service game,Clarity on requirements.,"Full time QA in games is primarily seen as unskilled work, leading to low wages and staff trying to use it as a springboard. Ultimately results in QA staff having generally low understanding of the state of software testing. Programmers don't generally spend significant time studying testing methodologies, as it's also a low paying specialization.",Less focus on unskilled QA and more focus on scalable methodologies. Testing in games should look substantially more like testing in tech companies. We rely on low wage unskilled labor to cover for our bad engineering practices.
3,['Designer/ProdOwner'],2,1,0,25-34,Canada,AA Game,"Game Designer, Level Designer, Gameplay Designer, and/or Product Owner",['Designer/ProdOwner'],Unreal Engine 4,1 year or less,100 - 499,$1M - $10M USD,"No, we did not have a publisher",2 - 3 years,"Our QA team is responsible for most of the daily testing on the game, we have a daily process where the QA team goes through the build and writes up tickets for issues that pop up while in gameplay. During this process the QA team goes through specific gameplay elements and content on a rotating schedule for total coverage.  Apart from that all developers were in charge for building automated tests for any content and/or systems they made. Additionally any content merges were put through an automated testing tool first.",5,4,5,5,5,2,4,4,"We have QA leads who draft up our testing process, I'm not fluent with the details but these plans are made early in the project and change according to the state and scale of the build.",5,-1,4,4,1,3,5,5,-1,"The goals in our testing processes are to identify regressions, bugs and tasks early and often as possible. All our tests as far as I understand them is to make sure we don't introduce issues with content merges and to make sure each larger change has QA eyes on them before merge. That's the best I can speak to in my domain since I'm not on the QA team.",5,5,5,3,"We use a healthy hybrid of automated and manual testing as far as I'm aware. As a content developer I use more automated tests before content merges. When I'm developing hot content or introducing larger changes, they always get manually and autotested first before a merge.",3,-1,4,4,2,5,5,5,-1,3,"Autotest merge tool, jira, and a few other first party validation tools for content merges.  The process is relatively painless apart from the fact that it does that up dev time to write the autotests for the content we make (which it makes up for in the long run). Jira is where we track any results from testing. I'm unsure about any other tools that QA uses.",3,4,4,4,4,5,"We have separate logs for separate pieces of content so they are easy to sort through. Sometimes logs might not indicate specific details for a tricky placement issue for example in which case we adapt the logging to present that information. Health and stability of build is dictated by the presence of game-breaking bugs. If the health of the build is really bad, devs hold of on merging content until the issues are resolved.  For specific feature value testing it'd mostly be in meeting format and non-automated.",5,4,5,4,1,5,I'm unsure of the details but our QA team is great at what they do and seem to have resources to help them do what they do.,5,5,3,4,-1,1,5,2,5,,"Frequent playtests both internally and externally, telemetry to objectively measure assumptions, quick surveys if we're looking for specific feedback, automated process for merging content","Ability to perform external playtests frequently to validate design assumptions, keeping up with autotests as the game evolves",Preset tests that come with an engine that can take care of root engine validation and expensive task processing
13,"['QA', 'Lead/Prod/Mgr', 'Release Manager, Live Ops Program Manager, Live Ops Manager']",0,1,9,35-44,Canada,AAA Game,Live Operations Program Manager / Release Manager,['Lead/Prod/Mgr'],Unreal,Over 3 years,100 - 499,Over $10M USD,"Yes, we had a publisher who provided marketing support",Over 3 years,"As a GaaS project, we had to take a pretty divergent path from traditional game dev when it came to testing and pushed for a Quality at Source approach to building and testing the game - meaning that we were always testing from day one as part of a continuous integration strategy and not relying on long testing periods at the end of each milestone to secure quality builds - our need instead was to maintain the ability to release at any time, which was broadly accomplished but still required some supplementary strategies for hardening features prior to release due to the complexity of the game.    We leaned pretty heavily on automation to provide the bulk of the testing work load with a small (for AAA) internal team dedicated largely to embedded testing within feature teams and for daily build validation on shipping builds. There was a heavy expectation on developers to write tests for their work up front and we had regular reviews of the automated tests to ensure we were getting the right coverage (our automated tests came in a wide range of forms and ranged from pretty typical unit tests to more extensive bootflow, performance, scale and visual tests).    We were also testing new versions of the game with the community at a regular cadence - initially just for short sessions in the early days moving to weekly and the 24/7 testing. Post release we had a separate product that we could release to for testing with a subset of the audience and every release had to first be staged through this environment before making it into the hands of regular players.",5,2,5,1,5,5,5,5,"The decision to go leverage Quality at Source/Continuous integration was taken pretty early on and that had an impact on how and when we planned tests.     Typically testing was a daily activity from the very first version of the game and testing planning broke down into the following areas:   - feature testing planning (i.e. how are features testing while in development)   - build validation planning (i.e. what tests need to be done to ensure that the current shipping version of the game is good to release)   - automation planning (i.e. what automated test suites do we need, what upkeep do we need to do on them and what additional testing capabilities should we invest in next)   - compliance test planning   - compatibility test planning   - localisation test planning   - accessibility test planning    For some of the latter, we had partners that we worked with to come up with the plans required but they were part of the same organisation as us for the most part.   -   ",5,4,5,5,1,4,2,2,2,This is a difficult question to answer because our goals for testing were pretty wide ranging (from experiential testing with the community to very stringent technical testing) but largely the testing process was very successful for the most part. It did require a lot of fine tuning though - when you take this kind of approach it can have an adverse affect on velocity and so we had to ensure that our tooling was doing the right things to supplement testing (i.e. allowing for togglable features or live configuration to be done in any environment) as well as a big investment in our build farm to run as many automated tests as we did.,5,5,5,4,Testing split was probably something along the lines of:  60% automated in-house  20% manual in-house (feature)  10% build validation in-house  20% testing done by players  ,1,4,5,2,2,5,5,5,5,3,"Azure DevOps (project management and bug database)  TeamCity (deployment and automated test orchestration)  Proprietary Microsoft tooling for crash reporting, telemetry, ops, publishing etc.  ",4,5,5,5,5,5,Testing output:  - Daily build reports via Email/MS teams (partly automated)  - Build dashboard  - Performance dashboards  - Telemetry dashboards  - Incident reports  - Azure DevOps dashboards,5,4,5,4,1,5,"By making testing a shared responsibility, our testing team internally was pretty small (around 8-10 at the most including management, largely focused on feature work) and outside of the immediate team we had a few small teams dedicated to build validation, compliance and localisation testing.",5,5,5,5,2,1,4,4,5,,"Automation, dynamic feature/game configuration",Localisation testing with procedurally generated text,"More quality at source and automation, less crunch and extended periods of polish testing at the end of projects."
9,['QA'],3,3,3,25-34,Canada,AAA Game,Quality Assurance and/or Game Tester,['QA'],Frostbite,1 year or less,500 or more,Over $10M USD,"Yes, we had a publisher who provided marketing support",Over 3 years,"Given how large the project was, and how distributed the QA group, I can only speak to what our team was doing (~100 testers/analysts).   - BVT: every morning, a small test ops group would complete a Build Verification Test on all systems and a cross section of content to ensure there were no blocking issues  - Compliance testing: as we approached submission to first party, we ran test passes against the certification requirements over the course of several weeks  - Loc testing: as localizations came in, we verified that they were hooked up correctly in game and were formatted properly  - VO testing: as voice overs came in, we verified that they played correctly in game, and matched the subtitles, for all supported languages  - Manual testing: given the scope of the game, there was always some kind of manual testing going on. This included boundary testing, progression testing, combat testing, core path testing, side quest testing, multiplayer testing, performance testing (on all supported platforms) and others I'm probably forgetting or was not asked to perform. In addition, everyone was expected to perform regression testing on their filed issues, and smoke checks around big fixes/breaks.",5,1,1,5,2,1,4,5,"Test plans were built by the leads and senior testers, I'm not sure when they were built. ",1,1,1,-1,1,-1,1,-1,-1,"We were testing for functionality, our testing process accomplished this by throwing as many bodies at the problem as we could afford and doing an extreme amount of overtime. The game was launched with minimal game breaking bugs, so I guess it worked. ",3,5,2,1,"I believe we had some automated testing, but my team was not privy to what those tests were testing for, or how/when they were run. Our team performed entirely manual testing for all aspects that we were asked to look at.",5,-1,-1,-1,-1,5,-1,5,1,5,"We used several in house built tools, all wrapped into one software package. This software allowed us to log crashes (and their frequency), as well as record sessions, record performance across sessions, file bugs and automatically attach logs/screenshots. There were more functions, but I was not trained on how to use them. The major benefits were that the system was custom built for the game we were testing, and had many excellent built in functionalities that made bug filing/investigation easier, but the major drawback was that if anything went wrong with the tool, it usually stayed broken for a very long time. ",5,3,5,4,5,4,"The testing output was highly dependent on which tester was writing the bug. We all went through the same training, but it stuck better for some people. If there were automated testing results, we never saw them. The people on the test ops group (my team) were generally more experienced, and had very few bugs come back for clarifications. ",-1,1,-1,5,-1,5,"Each testing pod had a dedicated lead, and at least a senior tester/analyst or two, depending on the focus. We also had a project manager dedicated to QA and a QA director. HR was covered by the general studio HR team, I was not privy to our funding specifications. We were authorized for as much OT as was determined to be necessary by studio leadership (again, I don't know who was making this call), and we always had the hardware we needed, though we did have some restrictions on how many consoles we could have (needed manager approval to have a console/get the permissions set up).",4,4,1,4,-1,2,4,1,1,"This project was brutal, and one of the best examples I've ever been in as far as ""traditional"" QA practices. There was a ton of burn out, a ton of overtime, and everyone got laid off at the end. ",Managers/leads who don't micro manage,"Being told that something isn't open for testing, after I've done a test pass on it","I would like more places to treat QA like they're part of the team, instead of some antagonizing force against them."
6,"['Dev/Prog/Eng', 'QA', 'Lead/Prod/Mgr', 'Designer/ProdOwner']",6,0,0,25-34,Canada,Indie Game,"Developer, Programmer, and/or Engineer","['QA', 'Lead/Prod/Mgr', 'Designer/ProdOwner']",Unity,1 year or less,1 - 49,Under $1M USD,"No, we did not have a publisher",1 year or less,"Mostly I would test as I was adding new features or mechanics, then I would make a build when I felt there was enough new stuff to test. I would then send it out to my teammates and some friends to get feedback on.",3,1,5,2,5,4,5,4,We did not make test plans,1,1,1,1,5,4,1,4,1,We were making sure the features and mechanics were working as intended and worked well in gameplay,5,5,5,4,We did not use automated testing,5,1,0,0,0,0,0,4,2,0,We did not use any tools for testing,0,0,0,0,0,4,We used debug.log() a lot,4,5,4,4,4,1,"We had no funding, so us on the team just tested it during our ""dev time"" using our Android devices we already owned. Same with the friends I asked to test it. ",4,2,1,3,2,1,5,5,5,It was a mobile game made by 3 people as our first commercial game,"Dedicating time to just testing a build, and if it's multiplayer then testing with others",Not being able to identify why the bugs are happening,Something that allows the tester to more easily identify why a bug is happening
3.75,"['QA', 'Lead/Prod/Mgr']",1,0,3,25-34,USA,AAA Game,Quality Assurance and/or Game Tester,['QA'],Proprietary,2 - 3 years,500 or more,Over $10M USD,"Yes, we had a publisher who provided marketing support",Over 3 years,Not sure how much I can say here.  ,4,1,4,5,5,5,5,5,Test plans were created both in office by leads and by a different qa  office under the same company. Used devsuite/devtrack.  ,5,3,4,4,1,2,4,5,4,Functionality ,5,5,5,3,"Majority manually, but automation saved us loads of time on things like invites to and from certain menus.",5,5,4,3,3,5,5,4,4,2,NDA,5,4,4,4,4,5,NDA,5,3,5,4,5,5,We had and have hundreds of testers on this project and it didn't/doesn't feel like enough sometimes.,2,4,4,4,4,1,5,1,1,,Direct communication with prod/devs,"Not having clear answers for tasks and needing to ask devs questions, but not having the ability to. ",More embedded and focused areas of testing. 
6,"['Dev/Prog/Eng', 'Designer/ProdOwner']",6,0,0,25-34,Canada,Indie Game,"Developer, Programmer, and/or Engineer",['Designer/ProdOwner'],Unity,2 - 3 years,1 - 49,Under $1M USD,"No, we did not have a publisher",Over 3 years,"Testing was typically performed at time of development. There was no dedicated QA department, rather, everyone was expected to test heavily before their work was integrated. Regular playtests by friends and family were also conducted.",4,1,1,1,2,2,5,5,There were no concrete test plans. Testing was done in an ad hoc manner by developers and designers as the project progressed.,1,1,1,1,5,5,1,1,1,"There were no formal testing goals. We were largely concerned with functionality, ensuring that all systems worked as intended without obvious failure states. There was a minor amount of testing done for quality/playability, but not much. This was also conducted informally.",2,5,3,5,"There was no automated testing. I believe I wrote a single script that scanned scenes for a common implementation mistake and fixed it automatically, but that is the closest to automated testing I can recall. All testing that was done was done manually.",5,1,0,0,0,4,0,5,1,0,"We used no tools for testing, other than the built-in Unity Profiler to manually assess performance in various scenarios.",5,5,5,5,0,1,"The profiler outputs a performance graph, showing proportional computing time required by each large-scale system (i.e. scripts, rendering, physics, etc.). For each rendered frame, there are also listed stats to drill down into how much computing time and memory each script used on a given frame. We used this information to pinpoint major performance concerns on each level of the game.",4,5,1,3,2,4,"No additional resources were allocated to testing. Testing work in progress was rolled into each developer and designer's duties. There were no dedicated testing machines, no cloud-based tools, nothing of the sort. For our personal duties, we weren't allotted a certain amount of developing and testing time - rather, we were expected to allocate our own time accordingly.",1,1,1,1,4,1,5,5,0,"In regard to testing, specifically, I believe that our lack of dedicated resources directly led to increased development time and a poor response upon release. Many obvious bugs that could have been discovered and fixed were left for our first customers to run into - some of them rendering the game unplayable. In addition to this, the quality of the game wasn't up to par with other games of a similar scope and budget in my opinion. Early user experience testing and a process for creating actionable items from feedback would have helped create a much better experience.","I think the easier something is to use, that is the more it ""just works"" out of the box, the happier I am with it. Any CI with complicated setup, and pages and pages of automated unit tests always end up being rather brittle. I don't want to spend an afternoon updating/debugging tests when I could be doing that on the game instead.","Lack of resources. Maintaining testing strategies and updating them as a project evolves takes time and resources that could go toward some other aspect of development. I think, with a larger team, that cost becomes easier to justify since it has an additive effect for every developer who's trying to integrate their work.","I would like to see some new automated build tools - something that doesn't clone a repository fresh from scratch every time it runs would be nice. I would also like to run automated performance tests on given sample scenes on a wide range of hardware (real, preferably, but simulated would be fine too). It would be nice to know how well my game runs on low to high end hardware without having to own all of it myself and go through the hassle of manually testing and profiling."
15,"['QA', 'Lead/Prod/Mgr']",0,0,10,35-44,Netherlands,AAA Game,"Team Lead, Producer, and/or Project Manager",['Lead/Prod/Mgr'],Proprietary,Over 3 years,100 - 499,Over $10M USD,"Yes, we had a publisher who provided marketing support",Over 3 years,"Limited unit tests were performed as part of continuous integration.  The rest of the testing was functional and non-functional manual greybox (assisted by debug tools), exploratory testing, confirmation testing (regressing the fixed bugs). The QA team was distributed among areas of ownership and often performed in-depth testing based on their knowledge of their associated features and content, as well as on request from developers.",5,1,5,5,4,5,5,2,Test strategy was developed by the QA manager in consultation with the test leads and production.     Test plans in the sense of having specific checks executed at specific times were only used for internal milestones like alpha and beta. The rest was mostly ad-hoc exploratory testing based on the needs of the project and constant communication with the development teams and production.,2,1,4,2,1,2,4,3,2,"Testing goals changed depending on the stage of the project as well as the implementation stage of certain types of content. In early stages it was focused on basic functionality and stability, while later on the criteria became increasingly stringent and expanded to include cosmetic aspects and behaviour in edge cases. The goals were discussed with production to make sure that testing remained relevant and efficient at every stage.  ",5,5,5,2,"We had an extensive suite of debug tools, which is not quite test automation, but it was immensely helpful. In terms of automated testing we had an AI-driven bot system that would run 24/7 to increase test coverage and trigger asserts and crashes.",5,1,2,5,1,5,2,4,2,-1,"Uh, we had over 150 debug views and tools in-engine, not going to list them all. Beyond that, we used Jira for bug tracking, TestRail for test planning and tracking, Confluence for documentation.",5,2,5,4,4,3,"Output was mostly bug reports in Jira addressed to developers. Additionally, the development team was tracking multiple data points via telemetry and using it to judge the quality of different areas, for example performance. Most of the data used was generated by the test team. Reporting was done for specific test passes in the form of an email write-up with Jira dashboard links and bug statistics.",4,3,4,4,2,4,"The internal testing team was a small unit (under 10), while the external testing team size varied with the needs of the project (up to 80 at peak). Staffing needs were discussed between the QA manager and the leads of the external teams, and submitted to executive management for budget approval. We sometimes got pushback on the budget, but not often. Hardware was requested as needed from the IT department and our requests were always fulfilled. In terms of time, QA provided estimates for larger test passes, and project schedule would be adjusted based on that. Sometimes production would have a specific deadline for specific testing, in which case we adjusted our task list to prioritize that.",4,4,2,4,5,3,3,1,5,"A flexible test strategy with an embedded test team, good test tools and good communication helped us ship a wildly successful project. And a lot of playtesting, but that was the domain of the user research team.",Debug tools ,Constant changes to the code and content,"Valuing QA more, both in including it as part of development and in supporting QA staff. Pay QA on the same level as other employees. QA are developers as well."
4,"['QA', 'Lead/Prod/Mgr']",2,0,2,25-34,United Kingdom,AAA Game,Quality Assurance and/or Game Tester,['QA'],Unreal 4,2 - 3 years,100 - 499,Over $10M USD,"Yes, we had a publisher who provided marketing support",Over 3 years,"Testing was split into various phases as required by the development team, and the phase at which the project was. Each day would be bookended by validation tests that would be sent to the publisher to ensure basic functionally was present in the days build. validation testing would encompass running through a preset list of in-game tasks the production team had deemed to be the basic functions the game would need to have functional for the build to ve considered acceptable for any further form of testing. Once validation had been confirmed and sent to the publisher, the test team would be prompted to regress the latest batch of fixes, stress test any new features etc. The days weren't very structured due to the volatile production process of the game in question - relying on a high degree of QA flexibility.",1,5,2,5,5,5,5,5,"Test plans where made by the QA manager with input from production. Sometimes senior QA would be asked to come up with test plans for ad-hoc situations, e.g. in a case where in house QA was asked to support localisation testing.",4,4,4,1,2,2,5,5,2,"The project was in development hell with a bug list too big for any of the attached development teams to keep hot with. Design had become an afterthought with the goal of just shipping the game. To this, the QA teams primary focus was just sanity tests and priority S & A issues (meaning crashes, soft locks, playtime blockers)",3,5,1,1,All but network testing on this particular QA team was manual.,5,1,0,5,-1,4,-1,4,0,-1,Describing the toolset would compromise the privacy of the project.,2,1,3,5,4,4,"Our specific QA team had very strict adherence to the QA managers bug template. The manager also had a habit of ""reprimanding"" any tester who'd fall out of line with the template. As the bug template had been agreed to by the wider team, we never had issues with the readability of our findings - the quality of our bugs was something we prided ourselves on when comparing out output to other QA teams on the project.",4,1,4,5,1,5,N/a ,4,4,2,4,2,2,5,1,4,,"A clear understanding of what should and shouldnt be tested. A lot of times, especially for new hires, it was rarely clear what the boundaries of what could be tested was. As our team was specifically focused on high priority bugs, it was easy to get in ""trouble"" for testing design related issues or similar. Clear parameters/test cases.","Bad tools that take time to onboard a test team with can vastly reduce bug output. But biggest pain points have to do with the treatment of QA as valued members of the games labour force. Our contracts made us easily replaceable, our input on any other forum but the bug database often dismissed","Clearer career progression within the discipline, and more respect for the people that do it (to be reflected in wages)"
10,"['Dev/Prog/Eng', 'Lead/Prod/Mgr', 'Designer/ProdOwner']",10,0,0,35-44,Canada,Indie Game,"Team Lead, Producer, and/or Project Manager","['Dev/Prog/Eng', 'QA', 'Lead/Prod/Mgr', 'Designer/ProdOwner']",Unity,1 year or less,1 - 49,Under $1M USD,"No, we did not have a publisher",1 year or less,"We did end user testing with our fan base over discord.    We also had weekly public beta tests (muffin Fight Mondays)    Prior to this, we had team game day testing on Friday every other week.  We had 4 to 8 play testers every 2 weeks.    We also have a QA submission process that developers can submit the game to for one off tests from our QA team.",5,4,5,4,1,1,2,5,"Designers create GDDs which capture requirements for production, which is used as a base line set of expectations    Our QA team has every QA release signed off by a designer, developer and QA person    Developers submit recommended test scripts to the QA team    End user tests and biweekly tests are recorded live on twitch, ualitative and observational for the QA team and lead developers.",5,5,5,1,2,4,5,4,4,"Our main goal is always end user experience.    We have documented process flows, usage flows.  Which are prevalidated by the QA team.  So our main user focus is purely usability, multiplayer sync, data sync type issues.    We omitted CI/CD, and Unit Testing.  In past experience, both can turn into make work projects for some projects - and weren't needed for this particular project.  We stopped using all three of these for any project shorter than 2 years.    We instead require the initial project team to define the base requirements checked throughout QA (base feature usage and workflow).    Our developers then complete their tickets in an agile method, and are responsible for submitting a build to our QA with a test plan to validate their performed work.    Finally the user biweekly game tests validate that the game is meeting our requirements.  Our focus here is not just bug fixes and general QA, but observing UX and game flow.  A lot of UX lessons we've learned for VR titles are not well documented and define our projects in cross industry.    We do have some checklists that have come out of past UX which are routinely validated but very difficult to unit test.  For example, all user input systems need to address all three core learning styles we focus on (kinetic, visual, audio).  Meaning vibrations, popups, and voice overs.    It also requires other UX requirements such as minimizing text (due to low resolution of some VR headsets), accessibility options such as our colour blind support, and the ability to display controller buttons in VR at all times.  We consider someone failing at a game due to the controls the fault of our company, failing at the game because someone plays better than them is what we want.  These outcomes and cause/effects are documented.    This is very different from games of ours such as Cultivate, which is a 3 year game with a much higher budget and much bigger team.  However Cultivate is still finishing development and so could not be included in this survey.",5,5,5,5,"We didn't include any automation for testing.    Testing was only manual for this title.  Strong visual titles dependant heavily on physics simulations and network states is notoriously difficult to create unit tests for.  As a result, I often find the work to creat the unit tests and adjusting them is usually far more (4 to 5x) what we see with manual testing.  And manual testing needs to be performed and documented anyway for both UX and for our marketing teams.    Unit testing can be valuable for common function testing, just as JSON structures, or basic math formula exchanges.  However most of this in/out style coding is now baked into the engines and less required for testing.  It is assumed, for example, what multiplying 2 vectors would produce.  It needs testing however to see how two complex colliders would interact with a live network feed running slow for one player.  Most of the transactional programming methodologies don't benefit from automated testing and are more effective to just have the pull requests peer reviewed for a project this size.    I'm very open to revising this over the course of this year, but haven't yet as we've just been overhwelmed with projects.  There is a huge value to unit testing but it's difficult to build in properly on short timelines and with new interns on the staff.",5,5,0,4,2,5,4,3,5,0,"Unity3D, core engine, well tested for base line game engine.  Functionality is not well tested beyond basic use however, and it requires very careful management of unity engine versions.  We have specific versions we use that have cleared our testing and been battle tested by us in the past.  Lately we've been moving more to Unreal, more for ethical reasons, but also improvement of visual quality.    nUnit for basic unit tests but mostly unused.  This is built into Unity, but doesn't do well with testing existing game states and mostly works on basic in/out functions (which are a lot less common in games these days as this functionality is built into the engines).  Also, as games are moving towards node based programming, this becomes less valuable for programmers and becomes more built into the engines.    Blender/Modo/Maya, Substance, Rokoko mocap & Adobe suite - creation of all models and assets.  This is mostly expensive and requires two stage reviews for ensuring all the models and animations look right and scale right and light properly.    VRTK - an open source VR framework.  This has been heavily modified by our team as it allows us to create mobile, VR, desktop and web games all in one.  VRTK is not well funded or supported these days, but was one of the only functional cross headset frameworks.      Finally we used twitch for live UA testing.    And we used our QA document process, it's literally just a form.  We have our QA led by non technical folks, so getting a PDF QA form makes it ideal for them to print, play and test.  We also do things such as hiring Blackfoot youth etc for our QA, and some of our key QA members at the time had limited network access and not strong knowledge of complex web tools. So we needed a simple clean document process.    Finally, we have an internal tool set that we use to validate and repair and check projects that we use.  ",4,4,5,4,4,5,"This depends on the type of testing    QA formal results come back as a formal document, tagged to that release and stored with notes and details of the test scripts provided.    Muffin Fight Mondays are observed by one of several key individuals and well documented for the team.  It needs to be approved by a business lead (as it incurs cost) and needs to fit the agile cycle without disrupting new features.    Our Friday team demos are observed and recorded the same way.    Finally, I do a final pass and coding cleanup myself for all projects.  Where I ensure the last few cycles are built exactly to red iron specifications.  This has no documented output and is probably both our weakest process for documentation but also the reason our projects do so well for quality and consistency  ",5,4,3,5,4,0,"For our QA reports, QA was handled by (2) resources with one person overseeing it.    Muffin Fight Mondays would usually involve (4) QA folks and our actual beta players.  With the QA lead managing the results.    Our bi-weekly play test was usually (4) developers, (4) QA, with the results being tracked by a Dev lead, Graphic lead, and QA lead.    Final QA approval, changes and final cycle is always managed by me as the CTO",5,5,2,2,1,1,5,4,5,None at this time!  Just that our process for larger games is different.,"For large projects, a well tracked QA process tied to our project ticket system is extremely valuable.  For smaller projects such as Muffin Fight, ensuring that we focus on QA at specific milestones is a better return on investment.  I think it needs to be analyzed per project.","It is difficult to auto test complex states, such as an active multiplayer game with physics.  ","Less focus on Unit Testing in of itself and better automated testing tied to actual functionality.  As we move to no-code graph based scripts, it becomes easy to tie in both unit testing for those but also to define a Unit test along side a complex function.  I'd like to see testing baked into systems clearer.  We we are moving away from Unity and more to Unreal, we expect our testing format to change considerably.  And we are likely to reintegrate CI/CD this year for larger projects.  But it's difficult to control a 20GB project full if complex assets, states, ML, AI, and phsycis systems and randomized rendering systems.  I'm not sure if our process is the right one but it places the users actual experience first and works for us for the time being"
10,"['Dev/Prog/Eng', 'QA', 'Designer/ProdOwner']",10,0,0,35-44,Canada,Indie Game,"Game Designer, Level Designer, Gameplay Designer, and/or Product Owner","['Dev/Prog/Eng', 'QA']",Proprietary,Over 3 years,1 - 49,Under $1M USD,"Yes, we had a publisher who provided marketing support",Over 3 years,As indies most of our QA was internal to the team. We had a closed beta test of approx. 20 individuals and eventually did about a week of professional QA (using an external team) before release.,5,1,5,3,4,4,2,4,"It was a dialog between our publisher and us as a team. It was purely based on what we thought made sense and the state of the project. How stable is the game? Any bugs were collected, filed and ordered by priority on trello. It was mostly up to me/the engineers on the team to decide what bugs to pick up and when.",0,1,4,1,4,5,1,0,1,Make the game as stable as possible before release and pass console certification.,5,5,5,3,We had some form of automated tests where it made sense. Validating data for correctness for example. But it was minimal.,5,1,1,5,3,5,2,4,4,1,We used Trello for tracking issues and their progress and that's it.,1,4,5,0,0,4,"I don't think this question applies to us. Testing didn't produce any ""data"" beyond bugs that needed fixing.",5,3,1,5,0,0,We invested in an external team for about 1 week of QA testing. This was mostly done to pass console certification. More would certainly have been nice.,2,2,2,2,4,2,5,5,5,QA/Testing was mostly a fluent process and not necessarily structured. As a small team it didn't make sense to invest the resources to do so. We were always aware of the state of the game. Having beta testers and internal team testing was enough to address the problems.,Having issues organized in trello or a spreadsheet is incredibly helpful.,Time constraints and not being able to reproduce issues.,Time is the biggest issue. Give me more time and I'll make you a better game.
16,"['QA', 'Lead/Prod/Mgr']",0,0,10,35-44,Canada,AAA Game,"Team Lead, Producer, and/or Project Manager",['QA'],Unreal,2 - 3 years,50 - 99,Over $10M USD,"Yes, we had a publisher who provided marketing support",2 - 3 years,"Divided into Tech (Functional) and Content (Qualitative) Test teams.     There were aspects of test case management for regression and session based testing for qualitative.      All checks were tracked where Functional had elements of Unit, smoke that also aided in governing when qualitative perform exploratory and ad-hoc.   ",4,4,0,0,4,4,-1,-1,"Embedded Analytics drafted the Strategies and plans with the approval of Leads.     these plans would be then converted into Cases and performed on a cadence by the Analysts.      As Testers came on board, some tests were diluted to suit, i.e external test groups.  While other Testers, also embedded will perform the scheduled regression testing.",5,3,5,5,1,2,5,2,2,Regression coverage and results aided exploratory coverage scheduling.  Can't provide feedback if isn't functional or can access the product but don't look at the non-functional components.      Goals are not bug count as bug count is the product of testing and not the expectations of testing.  Testing is never done but coverage vs yield rates determine if it's time to change focus.      ,5,5,5,4,Define Automated Testing.  Our Definition was anything that saves manual testing time.  A testhook or even applying tape to a button to make it auto fire is a form of automation as it allows the tester to focus efforts on testing requiring attention.    Another analogy of automation is to remove the tests that don't require human intervention or interpretations.  Normally these are highly functional and binary.     We strived to automate what can be scripted and those of priority also gated build generation.      Everything and every system had an isolated test level and the ability to test the system in isolation with known results and assets.  This aided root cause analysis and rule out core functionality errors from more compounding issues.    Auto test and functionality testing (actually any listed test script with binary results) fails to find exploratory issues ,5,4,4,4,3,5,1,3,5,2,"Test plan manager - the problem with linear test case tools is they don't dynamically change as the tester finds areas of risk.   Risk-based testing takes the tester to where it matters most and is usually supported by session-based testing.   Tools are not dynamic of this nature    A supportive tool was in-game bug identification and marking which allows a tester to see the bugs as they play and quickly knows if a section has had a lot of fix or a lot of failure.  Surfacing information of the game, in real time to the tester remove a lot of bug noise of time using bug filing tools to eliminate duplicate bug reports.",5,4,5,4,4,4,"Test coverage and management is the hardest to achieve without manual effort    Take your test structure (i.e what is functional and what is qualitative).  Make sure that test framework and its test results can be pivoted to answer:  Productions current structure of development.     - They want to know what is most broken or which teams velocity is introducing more issues    A system approach   - Developers care about the framework of the game so results should translate to their POV    The Game breakup  - Post ship has a completely new perspective on the game and that is how it's perceived by the public.  Make it so one can quickly answer the ""end game"" is buggy.    Always identify the factors that the universal.  Game levels tend to be a good one because a game level answers all three of the ways to pivot results because most game engines load/unload based on a level structure.  ",3,4,5,3,5,5,"Testers (the needs to know that coverage is adhered to and coverage like testing that consumes time i.e. playthroughs) are outsourced if necessary.  It's also wise to establish where and to how much debug they have and have limits.   Black box testing is a waste of time.  i.e. on an adjacent project the games were so long we always playthrough tested them at 5x game speed.   If it doesn't break at 5x, then it'll work fine at 1x.    Hardware distribution should account for hardware expectations of the end product.   Use Test kits that don't have the extra memory and PC configurations.      Build hardware libraries to shared peripherals, monitor types, and graphic cards.  QA needs to isolate specific tech issues but developers need the same hardware to develop a fix for them too.    Resource planning and allocation was planned and revised monthly and hiring fairs to bring in the right testers were done.   Take testing seriously and not say its just for anyone.    ",5,5,4,5,4,1,5,2,5,SFX3 had two previous titles and a lot of refinement before getting here.   The final stages and release of the product had a very little issue and was a high-quality deliverable.  ,"eliminate guess work on what something is working or broken in the way it is.  Documentation, testability in the sense of profiling and debug goes a long way.","Discipline and maturity in the game realm is difficult and the argument that exploratory testing is better because it finds the ""juicy"" bugs won't be possible without functional.  Separate endorphin receptors from logical coverage","Remove the about of influence from software.  Games are entertainment and the software component makes up less than 50%.  We should be taking more effort on testing ""fun"" as long as what functional is confirmed."
4,"['QA', 'Designer/ProdOwner']",2,1,1,25-34,Canada,AAA Game,Quality Assurance and/or Game Tester,['QA'],Frostbite,1 year or less,100 - 499,Over $10M USD,"Yes, we had a publisher who provided marketing support",Over 3 years,"Small team ran daily smoke/bvt. QA teams were broken down by feature (crit path, multiplayer, levels/exploration, gameplay, etc) with it being further broken down by specialization and ownership",4,2,-1,4,5,4,4,5,"Often created by leads or analysts and from there passed onto the testers as daily assignments. If changes needed to be made, these were directed to the lead who would update it (eventually)",2,2,3,-1,4,-1,4,5,-1,"Essentially checking off all the boxes to make sure things worked. Exploratory/ad-hoc testing was encouraged, but often this was not planned/scheduled and had to be done out of the tester's own curiosity or time. Many did not do this",2,4,3,1,"Huge amount of my experience was raw manpower manual testing. I know automated testing was done, but to what extent and how it was specifically executed is unknown. Basically a ""Someone is doing it, so dont worry about it"" thing",5,-1,-1,-1,-1,-1,-1,-1,-1,-1,"A lot of in-house tools. However, most tools were either ""This is old and doesn't work that well"" or ""This is new and it COULD be cool if it worked"". Documentation tools were completely neglected",3,2,5,4,4,4,"Followed a VERY specific format as outlined during training and on test plans. Straying from this very specific script was discouraged as it would ""confuse"" people ",4,-1,3,3,3,3,"Honestly, I'm not sure. Very much a ""cog in a big machine so do your work and maybe you get pizza"" kind of deal.",-1,4,-1,3,3,3,4,2,1,,Strong documentation from both the Testing and Developer side,"Huge focus on ""Bugs Filed Per Day"" stats. More quantity than quality bugs with no real approach to ad-hoc.","Less ""reactive"" testing and more ""proactive"". Have QA be part of of the process from inception to design to implementation for better knowledge on systems and mechanics."
4,"['Dev/Prog/Eng', 'Designer/ProdOwner']",2,1,1,35-44,Canada,Indie Game,"Game Designer, Level Designer, Gameplay Designer, and/or Product Owner",['Dev/Prog/Eng'],Proprietary,2 - 3 years,1 - 49,Under $1M USD,"Yes, we had a publisher, but they did not provide marketing support",Over 3 years,testing performed by internal QA; based on both internally documented bugs & player support submissions    verification of fixes also performed by internal QA    build verification also performed by internal QA prior to releases,4,1,4,5,4,4,3,5,"plans managed by internal QA, based on release schedule & current features in work",5,4,3,1,3,1,3,-1,4,no known bugs that are given a pass get pushed to the public build    no new regressions on each push,5,5,4,3,almost exclusively manual testing,3,5,5,3,3,5,2,5,2,5,in-house game state editor  manual labour,4,4,3,5,2,3,internal jira bug tasks    internal build verification reports,3,1,4,4,4,1,specific platforms & hardware setups    QA team; sometimes ringers called in from other teams,2,2,1,1,5,0,5,3,5,,"automated setup tools for specific states, reproducibility designed into the engine where RNG is involved","RNG, memory errors, weird player hardware setups",QA is game dev and a skilled dicipline. Anyone who says otherwise is wrong. VALUE YOUR QA AND PAY THEM BETTER.
,,,,,,(Additional Response),AA Game,"Team Lead, Producer, and/or Project Manager",['Designer/ProdOwner'],Proprietary,1 year or less,1 - 49,$1M - $10M USD,"No, we did not have a publisher",1 year or less,Basic testing on new features was done by developers and then passed to the QA department. The QA department would run smoke tests every morning and integration and regression tests on new features that came in.,4,2,5,5,2,5,5,5,"QA would work with developers to come up with a testing plan during feature development, so we could have a good sense of how a feature could be considered successful.",4,4,5,2,1,4,4,3,4,"We were testing to ensure that new features behaved adequately and in a positive manner. We didn't want to break the base functionality of the game, but also ensure that new features were positive improvements to the experience.",3,5,4,2,We primarily tested manually with a dedicated QA department. I'm unsure if automated testing happened.,5,2,3,4,3,3,3,5,2,3,We used Redmine for a bug backlog. We also used Jira to report bugs to developers for fixing and bug triage.,1,2,4,4,4,5,"QA testers would file bugs including specific reproduction steps, and attach any additional information (eg. recorded video) to a  Jira ticket. This would hit the main Jira backlog, at which point it would get triaged by severity and then either put into the selected for work backlog for developers to pick, or left in the backlog until the next triage. Triage happened once a week at the start of the project, and every day at the end of the project.",5,2,4,4,4,1,"We had a team of four dedicated QA testers for this project. They each had dedicated hardware for testing, including computers and consoles. There was one lead QA tester and a QA tester dedicated to each console (PS4, XBO, and Switch).",5,5,2,4,4,1,5,2,5,"It was really smooth testing. I think it was the best testing experience we had. Great testers, great process, and a great team who jumped on bugs quickly.",,,
,,,,,,(Additional Response),Indie Game,"Developer, Programmer, and/or Engineer",['Lead/Prod/Mgr'],Unity,2 - 3 years,1 - 49,Under $1M USD,"No, we did not have a publisher",Over 3 years,"The lead designer and product owner was largely responsible for thorough testing after milestones were met by the designers and developers. Developers were responsible for testing their implementation before integrating their work. We ran an automated build process every time work was merged into the main development branch - this proved costly as the team grew, however, so builds were eventually triggered manually at regular intervals instead.",1,3,5,1,4,4,5,5,"Integration test plans were handled by the product owner, while plans for unit testing were handled by the developers. The developers were also given the opportunity to request specific focus for the integration tests if they were concerned about a specific set of interactions.",4,5,5,4,2,2,1,3,4,"The primary concern for testing during the bulk of development was functionality. We accomplished this with integrated unit tests that ran as part of regular functionality and reported if unexpected results were seen. This was supported by build tests, where the product owner would play through content and determine if interactions between systems were working as intended and that no obvious bugs were discovered. Later on in the project lifecycle, testing turned toward user experience. This was accomplished by sending builds to a dedicated audience for early-access review. Note that during this time, the original testing methods were still in place so that the audience didn't have to waste time working around and reporting simple bugs.",5,5,5,4,"There was very little automated testing, though there was a reliance on automated reporting. Our mandate on the programming side of things dictated that we add a number of integrated tests that would ship with the game. Anything flagged by these tests were sent to a cloud reporting service that we could review. This was easier than trying to teach users how to find and send game logs to us manually, and caught important errors that may not have registered with a user when they occurred. Our major piece of truly automated testing was to run automated builds to ensure that there were no build errors or warnings when new work was integrated.",5,1,2,4,1,5,3,5,3,1,"We used Unity Cloud Build to provide up-to-date builds. The major benefit to this was that build issues were found immediately, while they were easy to diagnose. Builds done with far less frequency tend to fail on a number of issues, many of which may not be interrelated, and can be much harder to diagnose. The drawback, we found, was that it would re-download our binary content from Git LFS every time it ran, resulting in unsustainable costs once the development team grew large enough. Unity Cloud Diagnostics was also used once the project was given to a random assortment of users in early access. The major benefit here was getting automated, detailed reports of issues that otherwise wouldn't have been possible with manual reporting from this population. The downside was that it could only catch unhandled exceptions, or things that we manually added to test. Any logical errors would be left unexamined, particularly as it was often assumed that cloud-reported errors were the cause of bugs that users reported, even if this wasn't the case.",5,5,5,5,5,4,"Testing output was typically text-based logs. These were used to track down and fix issues that had gone otherwise unnoticed. With regard to cloud diagnostics, specifically, there was a graph which showed the frequency of a given issue, how many unique users were affected, and the hardware that was affected. All of the output was very usable, with the exception of the hardware labels on the graph output - hardware names are quite long, and the graph would never show enough to be legible. However, you could click into reports to get all of the information in an more legible fashion.",4,2,2,3,4,5,"No specific QA personnel were hired. Time was allocated by the product owner for testing, and developers were responsible for testing their work as part of their duties. Funds were spent on cloud build and diagnostic services provided by Unity. The total spend per month was around 1/50th of what an additional employee would have cost.",4,2,3,3,4,1,5,5,0,"Overall, I felt like we had a balanced approach that worked well. We accomplished a lot on a limited budget. I think that there was more we could have done to automate testing, and I would have liked to send major milestone builds to a QA firm, but I don't think we really had the resources to support either of those efforts. Having to choose between a dedicated QA employee who could plan and write tests/outsourcing and another full-time developer, I would have chosen another developer.",,,
,,,,,,(Additional Response),AAA Game,"Game Designer, Level Designer, Gameplay Designer, and/or Product Owner",['Dev/Prog/Eng'],Unreal,Over 3 years,100 - 499,Over $10M USD,"Yes, we had a publisher, but they did not provide marketing support",Over 3 years,We had a dedicated QA department. Developers were expected to write automated tests for each feature and for content. Automated tests were run on every check in. We were also encouraged to send builds to QA to check before submitting to the main development repo.,3,3,4,4,5,5,5,5,Developers would come up with their automated tests when designing systems and features. We would pass builds to QA to validate and when we did so provided validation criteria.,2,2,4,2,2,5,2,1,1,We were primarily concerned that new features or systems didn't break any existing features or systems. Automated testing was key to this process.,5,5,4,2,"We primarily relied on automated testing, with passoff to dedicated QA for smoke and regression tests. Integration tests were primarily automated.",1,1,5,2,4,3,5,5,1,5,"Unreal automated testing suite, proprietary testing tools before and after committing new code. Proprietary testing tools for deploying builds to QA and getting testing from large groups of people. The Unreal automated tests were run via our preflight tool before we committed changes. Our preflight tool also ran all of our unit tests, conducted content validation. Upon code commit, preflight would run again, but also include extended tests and performance tests. Every hour new builds of the accumulated changes would be deployed for QA to run regression and smoke tests on. ",5,5,5,5,5,5,Automated tests would specify which tests failed. It would also specify the particular item in the test logic that failed. Regression and Smoke tests by the QA department would yield specific and reproducible test steps that developers could look at in order to fix bugs and defects.,4,1,4,4,2,4,"We heavily invested in all aspects of testing resources. There were developers dedicated to our internal testing tools, we had dedicated QA personnel (although probably not enough).    The one downside is that there was enormous pressure to put new features and content into the game, which made it hard for individual developers to maintain automated tests for their features and systems.",4,4,2,4,4,2,4,1,5,"I would say we handled automated testing very wrong on this project. We had bad ideas about what automated testing should be when we started, and this unfortunately persisted during development. I think we would have been better served if we had been given more time and space to update our automated tests, or more care and resources were dedicated to helping developers and designers implement their tests, rather than having them figure it out as they go along. Reflecting on this I feel bad that I let testing fall by the wayside because it became hard and unruly to manage a giant test backlog for big systems and features.",,,